{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Which NIPS papers are similar? *[a simple graphlab knn & tf-idf exercise]*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal: Find the papers that are similar based on abstract and full-text\n",
    "### Steps:\n",
    "1. Find the important keywords of each document using tf-idf\n",
    "2. Apply knn_model on tf-idf to find similar papers\n",
    "\n",
    "### Cleaning: \n",
    "* Clean text from \\n \\x and things like that by \n",
    "    1. Replace \\n and \\x0c with space\n",
    "    2. Apply unicode\n",
    "    3. Make everything lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import graphlab\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's discover the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import data using pandas and put into SFrames:\n",
    "papers_df = pd.read_csv('Data/output/Papers.csv')\n",
    "papers_data = graphlab.SFrame(data = papers_df)\n",
    "authors_df = pd.read_csv('Data/output/Authors.csv')\n",
    "authors_data = graphlab.SFrame(data = authors_df)\n",
    "authorId_df = pd.read_csv('Data/output/PaperAuthors.csv')\n",
    "authorId_data = graphlab.SFrame(data = authorId_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\"><table frame=\"box\" rules=\"cols\">\n",
       "    <tr>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">Id</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">Title</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">EventType</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">PdfName</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">Abstract</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">5677</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Double or Nothing:<br>Multiplicative Incentive ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Poster</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">5677-double-or-nothing-<br>multiplicative-incent ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Crowdsourcing has gained<br>immense popularity in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">5941</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Learning with Symmetric<br>Label Noise: The ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Spotlight</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">5941-learning-with-<br>symmetric-label-noise- ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Convex potential<br>minimisation is the de ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">6019</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Algorithmic Stability and<br>Uniform Generalization ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Poster</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">6019-algorithmic-<br>stability-and-uniform- ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">One of the central<br>questions in statistical ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">6035</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Adaptive Low-Complexity<br>Sequential Inference for ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Poster</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">6035-adaptive-low-<br>complexity-sequential- ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">We develop a sequential<br>low-complexity inference ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">5978</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Covariance-Controlled<br>Adaptive Langevin ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Poster</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">5978-covariance-<br>controlled-adaptive- ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Monte Carlo sampling for<br>Bayesian posterior ...</td>\n",
       "    </tr>\n",
       "</table>\n",
       "<table frame=\"box\" rules=\"cols\">\n",
       "    <tr>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">PaperText</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Double or Nothing:<br>Multiplicative\\nIncen ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Learning with Symmetric<br>Label Noise: ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Algorithmic Stability and<br>Uniform ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Adaptive Low-Complexity<br>Sequential Inference ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Covariance-Controlled<br>Adaptive ...</td>\n",
       "    </tr>\n",
       "</table>\n",
       "[5 rows x 6 columns]<br/>\n",
       "</div>"
      ],
      "text/plain": [
       "Columns:\n",
       "\tId\tint\n",
       "\tTitle\tstr\n",
       "\tEventType\tstr\n",
       "\tPdfName\tstr\n",
       "\tAbstract\tstr\n",
       "\tPaperText\tstr\n",
       "\n",
       "Rows: 5\n",
       "\n",
       "Data:\n",
       "+------+-------------------------------+-----------+\n",
       "|  Id  |             Title             | EventType |\n",
       "+------+-------------------------------+-----------+\n",
       "| 5677 | Double or Nothing: Multipl... |   Poster  |\n",
       "| 5941 | Learning with Symmetric La... | Spotlight |\n",
       "| 6019 | Algorithmic Stability and ... |   Poster  |\n",
       "| 6035 | Adaptive Low-Complexity Se... |   Poster  |\n",
       "| 5978 | Covariance-Controlled Adap... |   Poster  |\n",
       "+------+-------------------------------+-----------+\n",
       "+-------------------------------+-------------------------------+\n",
       "|            PdfName            |            Abstract           |\n",
       "+-------------------------------+-------------------------------+\n",
       "| 5677-double-or-nothing-mul... | Crowdsourcing has gained i... |\n",
       "| 5941-learning-with-symmetr... | Convex potential minimisat... |\n",
       "| 6019-algorithmic-stability... | One of the central questio... |\n",
       "| 6035-adaptive-low-complexi... | We develop a sequential lo... |\n",
       "| 5978-covariance-controlled... | Monte Carlo sampling for B... |\n",
       "+-------------------------------+-------------------------------+\n",
       "+-------------------------------+\n",
       "|           PaperText           |\n",
       "+-------------------------------+\n",
       "| Double or Nothing: Multipl... |\n",
       "| Learning with Symmetric La... |\n",
       "| Algorithmic Stability and ... |\n",
       "| Adaptive Low-Complexity Se... |\n",
       "| Covariance-Controlled Adap... |\n",
       "+-------------------------------+\n",
       "[5 rows x 6 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\"><table frame=\"box\" rules=\"cols\">\n",
       "    <tr>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">Id</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">Name</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">4113</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Constantine Caramanis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">4828</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Richard L. Lewis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">5506</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Ryan Kiros</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">7331</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Kfir Levy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">8429</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Wei Cao</td>\n",
       "    </tr>\n",
       "</table>\n",
       "[5 rows x 2 columns]<br/>\n",
       "</div>"
      ],
      "text/plain": [
       "Columns:\n",
       "\tId\tint\n",
       "\tName\tstr\n",
       "\n",
       "Rows: 5\n",
       "\n",
       "Data:\n",
       "+------+-----------------------+\n",
       "|  Id  |          Name         |\n",
       "+------+-----------------------+\n",
       "| 4113 | Constantine Caramanis |\n",
       "| 4828 |    Richard L. Lewis   |\n",
       "| 5506 |       Ryan Kiros      |\n",
       "| 7331 |       Kfir Levy       |\n",
       "| 8429 |        Wei Cao        |\n",
       "+------+-----------------------+\n",
       "[5 rows x 2 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "authors_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\"><table frame=\"box\" rules=\"cols\">\n",
       "    <tr>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">Id</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">PaperId</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">AuthorId</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">5677</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">7956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">2</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">5677</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">2649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">3</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">5941</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">8299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">4</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">5941</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">8300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">5</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">5941</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">575</td>\n",
       "    </tr>\n",
       "</table>\n",
       "[5 rows x 3 columns]<br/>\n",
       "</div>"
      ],
      "text/plain": [
       "Columns:\n",
       "\tId\tint\n",
       "\tPaperId\tint\n",
       "\tAuthorId\tint\n",
       "\n",
       "Rows: 5\n",
       "\n",
       "Data:\n",
       "+----+---------+----------+\n",
       "| Id | PaperId | AuthorId |\n",
       "+----+---------+----------+\n",
       "| 1  |   5677  |   7956   |\n",
       "| 2  |   5677  |   2649   |\n",
       "| 3  |   5941  |   8299   |\n",
       "| 4  |   5941  |   8300   |\n",
       "| 5  |   5941  |   575    |\n",
       "+----+---------+----------+\n",
       "[5 rows x 3 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "authorId_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Learning with Symmetric Label Noise: The\\nImportance of Being Unhinged\\n\\nBrendan van Rooyen\\xe2\\x88\\x97,\\xe2\\x80\\xa0\\n\\xe2\\x88\\x97\\n\\nAditya Krishna Menon\\xe2\\x80\\xa0,\\xe2\\x88\\x97\\n\\nThe Australian National University\\n\\n\\xe2\\x80\\xa0\\n\\nRobert C. Williamson\\xe2\\x88\\x97,\\xe2\\x80\\xa0\\n\\nNational ICT Australia\\n\\n{ brendan.vanrooyen, aditya.menon, bob.williamson }@nicta.com.au\\n\\nAbstract\\nConvex potential minimisation is the de facto approach to binary classification.\\nHowever, Long and Servedio [2010] proved that under symmetric label noise\\n(SLN), minimisation of any convex potential over a linear function class can result in classification performance equivalent to random guessing. This ostensibly\\nshows that convex losses are not SLN-robust. In this paper, we propose a convex,\\nclassification-calibrated loss and prove that it is SLN-robust. The loss avoids the\\nLong and Servedio [2010] result by virtue of being negatively unbounded. The\\nloss is a modification of the hinge loss, where one does not clamp at zero; hence,\\nwe call it the unhinged loss. We show that the optimal unhi'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example Before Cleaning:\n",
    "papers_data[1]['PaperText'][0:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Abstract and PaperText:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_text(text):\n",
    "    list_of_cleaning_signs = ['\\x0c', '\\n']\n",
    "    for sign in list_of_cleaning_signs:\n",
    "        text = text.replace(sign, ' ')\n",
    "    text = unicode(text, errors='ignore')\n",
    "    clean_text = re.sub('[^a-zA-Z]+', ' ', text)\n",
    "    return clean_text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "papers_data['PaperText_clean'] = papers_data['PaperText'].apply(lambda x: clean_text(x))\n",
    "papers_data['Abstract_clean'] = papers_data['Abstract'].apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'learning with symmetric label noise the importance of being unhinged brendan van rooyen aditya krishna menon the australian national university robert c williamson national ict australia brendan vanrooyen aditya menon bob williamson nicta com au abstract convex potential minimisation is the de facto approach to binary classification however long and servedio proved that under symmetric label noise sln minimisation of any convex potential over a linear function class can result in classification performance equivalent to random guessing this ostensibly shows that convex losses are not sln robust in this paper we propose a convex classification calibrated loss and prove that it is sln robust the loss avoids the long and servedio result by virtue of being negatively unbounded the loss is a modification of the hinge loss where one does not clamp at zero hence we call it the unhinged loss we show that the optimal unhinged solution is equivalent to that of a strongly regularised svm and is t'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example After Cleaning\n",
    "papers_data[1]['PaperText_clean'][0:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build tf-idf columns for Abstract and PaperText:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "papers_data['word_count_abstract_clean'] = graphlab.text_analytics.count_words(papers_data['Abstract_clean'])\n",
    "papers_data['tf_idf_Abstract_clean'] = graphlab.text_analytics.tf_idf(papers_data['word_count_abstract_clean'])\n",
    "papers_data['word_count_papertext_clean'] = graphlab.text_analytics.count_words(papers_data['PaperText_clean'])\n",
    "papers_data['tf_idf_PaperText_clean'] = graphlab.text_analytics.tf_idf(papers_data['word_count_papertext_clean'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a function that takes paperID as input and prints keywords sorted by tf-idf:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def given_paperID_give_keywords(papers_data, paper_id, tfidf_col_name):\n",
    "    paper = papers_data[papers_data['Id']== paper_id]\n",
    "    keywords = paper[[tfidf_col_name]].stack(tfidf_col_name,\n",
    "                                             new_column_name=['word', tfidf_col_name]).sort(tfidf_col_name,\n",
    "                                                                                            ascending=False)\n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keywords based on Abstract:\n",
      "+----------------+-----------------------+\n",
      "|      word      | tf_idf_Abstract_clean |\n",
      "+----------------+-----------------------+\n",
      "|      sln       |     29.9946828097     |\n",
      "|    unhinged    |     17.9968096858     |\n",
      "|      loss      |     16.2185981757     |\n",
      "|   potential    |     13.1684959485     |\n",
      "|    servedio    |     11.9978731239     |\n",
      "|  minimisation  |     11.9978731239     |\n",
      "|     convex     |      10.143223242     |\n",
      "|     robust     |     8.22252007178     |\n",
      "|   equivalent   |     7.83899004053     |\n",
      "| classification |      6.9301713235     |\n",
      "+----------------+-----------------------+\n",
      "[10 rows x 2 columns]\n",
      "\n",
      "Keywords based on PaperText:\n",
      "+------------+------------------------+\n",
      "|    word    | tf_idf_PaperText_clean |\n",
      "+------------+------------------------+\n",
      "|    sln     |     515.908544327      |\n",
      "|  unhinged  |     263.953208726      |\n",
      "|    unh     |     107.980858115      |\n",
      "|  scorers   |     106.115787628      |\n",
      "|    flin    |     83.9851118673      |\n",
      "|   scorer   |     77.9861753053      |\n",
      "| robustness |     76.9698099772      |\n",
      "|  servedio  |     61.4529810932      |\n",
      "|    wunh    |     59.9893656195      |\n",
      "|    loss    |     51.6815648881      |\n",
      "+------------+------------------------+\n",
      "[10 rows x 2 columns]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example for keywords based on abstract\n",
    "paper_id_example = 5941\n",
    "print \"Keywords based on Abstract:\"\n",
    "print given_paperID_give_keywords(papers_data, paper_id_example, 'tf_idf_Abstract_clean').head(10)\n",
    "print \"Keywords based on PaperText:\"\n",
    "print given_paperID_give_keywords(papers_data, paper_id_example, 'tf_idf_PaperText_clean').head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a function that takes paperID and knn_model as input and gives similar paper's Ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " def given_paperID_give_similar_papersID(knnModel, papers_data, paper_id):\n",
    "    similar_paper_ids = knnModel.query(papers_data[papers_data['Id']== paper_id], verbose=False)['reference_label']\n",
    "    return similar_paper_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROGRESS: Starting brute force nearest neighbors model training.\n",
      "PROGRESS: Starting brute force nearest neighbors model training.\n"
     ]
    }
   ],
   "source": [
    "knn_model_Abstract = graphlab.nearest_neighbors.create(papers_data, features=['tf_idf_Abstract_clean'], label='Id')\n",
    "knn_model_PaperText = graphlab.nearest_neighbors.create(papers_data, features=['tf_idf_PaperText_clean'], label='Id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar papers based on Abstract:\n",
      "[5941, 5742, 5801, 5924, 5745]\n",
      "Similar papers based on PaperText:\n",
      "[5941, 5999, 5994, 5921, 5806]\n"
     ]
    }
   ],
   "source": [
    "paper_id_example = 5941\n",
    "Abstract_sim_papers_example = given_paperID_give_similar_papersID(knn_model_Abstract, papers_data, paper_id_example)\n",
    "PaperText_sim_papers_example = given_paperID_give_similar_papersID(knn_model_PaperText, papers_data, paper_id_example)\n",
    "print \"Similar papers based on Abstract:\"\n",
    "print Abstract_sim_papers_example\n",
    "print \"Similar papers based on PaperText:\"\n",
    "print PaperText_sim_papers_example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some post-processing functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def given_paperID_give_authours_id(paper_id, author_data, author_id_data):\n",
    "    id_author_list = author_id_data[author_id_data['PaperId']==paper_id]['AuthorId']\n",
    "    return id_author_list\n",
    "\n",
    "def given_authorID_give_name(author_id, author_data):\n",
    "    author_name = author_data[author_data['Id'] == author_id]['Name'][0]\n",
    "    return author_name\n",
    "\n",
    "def given_similar_paperIDs_give_their_titles(sim_papers_list, paper_data):\n",
    "    titles = []\n",
    "    for paper_id in sim_papers_list:\n",
    "        titles.append(paper_data[paper_data['Id']==paper_id]['Title'][0]+'.')\n",
    "    return titles\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's check the title of similar papers for the example case based on abstract and PaperText:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title of similar papers based on Abstract:\n",
      "\n",
      "\n",
      "Learning with Symmetric Label Noise: The Importance of Being Unhinged.\n",
      "Top-k Multiclass SVM.\n",
      "Reflection, Refraction, and Hamiltonian Monte Carlo.\n",
      "A Dual Augmented Block Minimization Framework for Learning with Limited Memory.\n",
      "Distributionally Robust Logistic Regression.\n"
     ]
    }
   ],
   "source": [
    "print \"Title of similar papers based on Abstract:\\n\\n\"\n",
    "for title in given_similar_paperIDs_give_their_titles(Abstract_sim_papers_example, papers_data):\n",
    "    print title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title of similar papers based on PaperText:\n",
      "\n",
      "\n",
      "Learning with Symmetric Label Noise: The Importance of Being Unhinged.\n",
      "Fast Classification Rates for High-dimensional Gaussian Generative Models.\n",
      "Online F-Measure Optimization.\n",
      "Convergence Rates of Active Learning for Maximum Likelihood Estimation.\n",
      "On the Accuracy of Self-Normalized Log-Linear Models.\n"
     ]
    }
   ],
   "source": [
    "print \"Title of similar papers based on PaperText:\\n\\n\"\n",
    "for title in given_similar_paperIDs_give_their_titles(PaperText_sim_papers_example, papers_data):\n",
    "    print title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's get the authors_name for each similar paper to the example case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar Paper Ids and their author's names based on PaperText:\n",
      "\n",
      "\n",
      "[5941, 'Brendan van Rooyen', 'Aditya Menon', 'Robert C. Williamson']\n",
      "[5999, 'Tianyang Li', 'Adarsh Prasad', 'Pradeep K. Ravikumar']\n",
      "[5994, 'R\\xc3\\xb3bert Busa-Fekete', 'Bal\\xc3\\xa1zs Sz\\xc3\\xb6r\\xc3\\xa9nyi', 'Krzysztof Dembczynski', 'Eyke H\\xc3\\xbcllermeier']\n",
      "[5921, 'Kamalika Chaudhuri', 'Sham M. Kakade', 'Praneeth Netrapalli', 'Sujay Sanghavi']\n",
      "[5806, 'Jacob Andreas', 'Maxim Rabinovich', 'Michael I. Jordan', 'Dan Klein']\n"
     ]
    }
   ],
   "source": [
    "# Just to see if the given_authorID_give_name works\n",
    "print \"Similar Paper Ids and their author's names based on PaperText:\\n\\n\"\n",
    "for paper_id in PaperText_sim_papers_example:\n",
    "    authors_id_list = given_paperID_give_authours_id(paper_id, authors_data, authorId_data)\n",
    "    authors_name_list = [paper_id]\n",
    "    for auth_id in authors_id_list:\n",
    "        authors_name_list.append(given_authorID_give_name(auth_id, authors_data))\n",
    "    print authors_name_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's print the abstract of similar papers for each model [to see if they are similar???]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abstract of similar papers based on Abstract:\n",
      "\n",
      "\n",
      "Convex potential minimisation is the de facto approach to binary classification. However, Long and Servedio [2008] proved that under symmetric label noise (SLN), minimisation of any convex potential over a linear function class can result in classification performance equivalent to random guessing. This ostensibly shows that convex losses are not SLN-robust. In this paper, we propose a convex, classification-calibrated loss and prove that it is SLN-robust. The loss avoids the Long and Servedio [2008] result by virtue of being negatively unbounded. The loss is a modification of the hinge loss, where one does not clamp at zero; hence, we call it the unhinged loss. We show that the optimal unhinged solution is equivalent to that of a strongly regularised SVM, and is the limiting solution for any convex potential; this implies that strong l2 regularisation makes most standard learners SLN-robust. Experiments confirm the unhinged loss’ SLN-robustness.\n",
      "\n",
      "\n",
      "\n",
      "Class ambiguity is typical in image classification problems with a large number of classes. When classes are difficult to discriminate, it makes sense to allow k guesses and evaluate classifiers based on the top-k error instead of the standard zero-one loss. We propose top-k multiclass SVM as a direct method to optimize for top-k performance. Our generalization of the well-known multiclass SVM is based on a tight convex upper bound of the top-k error. We propose a fast optimization scheme based on an efficient projection onto the top-k simplex, which is of its own interest. Experiments on five datasets show consistent improvements in top-k accuracy compared to various baselines.\n",
      "\n",
      "\n",
      "\n",
      "Hamiltonian Monte Carlo (HMC) is a successful approach for sampling from continuous densities. However, it has difficulty simulating Hamiltonian dynamics with non-smooth functions, leading to poor performance. This paper is motivated by the behavior of Hamiltonian dynamics in physical systems like optics. We introduce a modification of the Leapfrog discretization of Hamiltonian dynamics on piecewise continuous energies, where intersections of the trajectory with discontinuities are detected, and the momentum is reflected or refracted to compensate for the change in energy. We prove that this method preserves the correct stationary distribution when boundaries are affine. Experiments show that by reducing the number of rejected samples, this method improves on traditional HMC.\n",
      "\n",
      "\n",
      "\n",
      "In past few years, several techniques have been proposed for training of linear Support Vector Machine (SVM) in limited-memory setting, where a dual block-coordinate descent (dual-BCD) method was used to balance cost spent on I/O and computation. In this paper, we consider the more general setting of regularized \\emph{Empirical Risk Minimization (ERM)} when data cannot fit into memory. In particular, we generalize the existing block minimization framework based on strong duality and \\emph{Augmented Lagrangian} technique to achieve global convergence for ERM with arbitrary convex loss function and regularizer. The block minimization framework is flexible in the sense that, given a solver working under sufficient memory, one can integrate it with the framework to obtain a solver globally convergent under limited-memory condition. We conduct experiments on L1-regularized classification and regression problems to corroborate our convergence theory and compare the proposed framework to  algorithms adopted from online and distributed settings, which shows superiority of the proposed approach on data of size ten times larger than the memory capacity.\n",
      "\n",
      "\n",
      "\n",
      "This paper proposes a distributionally robust approach to logistic regression. We use the Wasserstein distance to construct a ball in the space of probability distributions centered at the uniform distribution on the training samples. If the radius of this Wasserstein ball is chosen judiciously, we can guarantee that it contains the unknown data-generating distribution with high confidence. We then formulate a distributionally robust logistic regression model that minimizes a worst-case expected logloss function, where the worst case is taken over all distributions in the Wasserstein ball. We prove that this optimization problem admits a tractable reformulation and encapsulates the classical as well as the popular regularized logistic regression problems as special cases. We further propose a distributionally robust approach based on Wasserstein balls to compute upper and lower confidence bounds on the misclassification probability of the resulting classifier. These bounds are given by the optimal values of two highly tractable linear programs. We validate our theoretical out-of-sample guarantees through simulated and empirical experiments.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print \"Abstract of similar papers based on Abstract:\\n\\n\"\n",
    "tfidf_col_name = 'tf_idf_Abstract_clean'\n",
    "for paper_id in Abstract_sim_papers_example:\n",
    "    print papers_data[papers_data['Id']==paper_id]['Abstract'][0]\n",
    "    print '\\n\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abstract of similar papers based on PaperText:\n",
      "\n",
      "\n",
      "Convex potential minimisation is the de facto approach to binary classification. However, Long and Servedio [2008] proved that under symmetric label noise (SLN), minimisation of any convex potential over a linear function class can result in classification performance equivalent to random guessing. This ostensibly shows that convex losses are not SLN-robust. In this paper, we propose a convex, classification-calibrated loss and prove that it is SLN-robust. The loss avoids the Long and Servedio [2008] result by virtue of being negatively unbounded. The loss is a modification of the hinge loss, where one does not clamp at zero; hence, we call it the unhinged loss. We show that the optimal unhinged solution is equivalent to that of a strongly regularised SVM, and is the limiting solution for any convex potential; this implies that strong l2 regularisation makes most standard learners SLN-robust. Experiments confirm the unhinged loss’ SLN-robustness.\n",
      "\n",
      "\n",
      "\n",
      "We consider the problem of binary classification when the covariates conditioned on the each of the response values follow multivariate Gaussian distributions. We focus on the setting where the covariance matrices for the two conditional distributions are the same. The corresponding generative model classifier, derived via the Bayes rule, also called Linear Discriminant Analysis, has been shown to behave poorly in high-dimensional settings. We present a novel analysis of the classification error of any linear discriminant approach given conditional Gaussian models. This allows us to compare the generative model classifier, other recently proposed discriminative approaches that directly learn the discriminant function, and then finally logistic regression which is another classical discriminative model classifier. As we show, under a natural sparsity assumption, and letting $s$ denote the sparsity of the Bayes classifier, $p$ the number of covariates, and $n$ the number of samples, the simple ($\\ell_1$-regularized) logistic regression classifier achieves the fast misclassification error rates of $O\\left(\\frac{s \\log p}{n}\\right)$, which is much better than the other approaches, which are either inconsistent under high-dimensional settings, or achieve a slower rate of $O\\left(\\sqrt{\\frac{s \\log p}{n}}\\right)$.\n",
      "\n",
      "\n",
      "\n",
      "The F-measure is an important and commonly used performance metric for binary prediction tasks. By combining precision and recall into a single score, it avoids disadvantages of simple metrics like the error rate, especially in cases of imbalanced class distributions. The problem of optimizing the F-measure, that is, of developing learning algorithms that perform optimally in the sense of this measure, has recently been tackled by several authors. In this paper, we study the problem of F-measure maximization in the setting of online learning. We propose an efficient online algorithm and provide a formal analysis of its convergence properties. Moreover, first experimental results are presented, showing that our method performs well in practice.\n",
      "\n",
      "\n",
      "\n",
      "An active learner is given a class of models, a large set of unlabeled examples, and the ability to interactively query labels of a subset of these examples; the goal of the learner is to learn a model in the class that fits the data well. Previous theoretical work has rigorously characterized label complexity of active learning, but most of this work has focused on the PAC or the agnostic PAC model. In this paper, we shift our attention to a more general setting -- maximum likelihood estimation. Provided certain conditions hold on the model class, we provide a two-stage active learning algorithm for this problem. The conditions we require are fairly general, and cover the widely popular class of Generalized Linear Models, which in turn, include models for binary and multi-class classification, regression, and conditional random fields. We provide an upper bound on the label requirement of our algorithm, and a lower bound that matches it up to lower order terms. Our analysis shows that unlike binary classification in the realizable case, just a single extraround of interaction is sufficient to achieve near-optimal performance in maximum likelihood estimation.  On the empirical side, the recent work in (Gu et al. 2012) and (Gu et al. 2014) (on active linear and logistic regression) shows the promise of this approach.\n",
      "\n",
      "\n",
      "\n",
      "Calculation of the log-normalizer is a major computational obstacle in applications of log-linear models with large output spaces. The problem of fast normalizer computation has therefore attracted significant attention in the theoretical and applied machine learning literature. In this paper, we analyze a recently proposed technique known as ``self-normalization'', which introduces a regularization term in training to penalize log normalizers for deviating from zero. This makes it possible to use unnormalized model scores as approximate probabilities. Empirical evidence suggests that self-normalization is extremely effective, but a theoretical understanding of why it should work, and how generally it can be applied, is largely lacking.We prove upper bounds on the loss in accuracy due to self-normalization, describe classes of input distributionsthat self-normalize easily, and construct explicit examples of high-variance input distributions. Our theoretical results make predictions about the difficulty of fitting  self-normalized models to several classes of distributions, and we conclude with empirical validation of these predictions on both real and synthetic datasets.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print \"Abstract of similar papers based on PaperText:\\n\\n\"\n",
    "tfidf_col_name = 'tf_idf_PaperText_clean'\n",
    "for paper_id in PaperText_sim_papers_example:\n",
    "    print papers_data[papers_data['Id']==paper_id]['Abstract'][0]\n",
    "    print '\\n\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *** Questions & notes: ***\n",
    "1. Are these papers really similar? i.e. Is there an automated way to evaluate?\n",
    "\n",
    "2. Maybe we can check if they referenced the same papers? \n",
    "\n",
    "3. Which model is better? Abstract or PaperText?\n",
    "\n",
    "4. How about using different type of distance to find closest paper?\n",
    "\n",
    "5. Since we are working on a single conference, maybe the papers are too similar to be distinguished using this method? More explanation: Say, we have some number of Fluid-mechanics, ML, sport, ... related papers; Since the terminology is very different between these groups, it is easier to find number of similar papers over the entire data compare to finding similar papers within a each group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *** Important note: ***\n",
    "* All graphlab-related materials in this notebook follow the materials explained on the *** \"Machine Learning Foundations: A Case Study Approach\" *** course on Coursera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
