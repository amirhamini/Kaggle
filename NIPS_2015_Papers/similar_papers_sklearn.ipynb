{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Which NIPS papers are similar? *[a simple sklearn knn & tf-idf exercise]*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal: Find the papers that are similar based on abstract and full-text\n",
    "### Steps:\n",
    "1. Find the important keywords of each document using tf-idf\n",
    "2. Apply knn_model on tf-idf to find similar papers\n",
    "\n",
    "### Cleaning: \n",
    "* Clean text from \\n \\x and things like that by \n",
    "    1. Replace \\n and \\x0c with space\n",
    "    2. Apply unicode\n",
    "    3. Make everything lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/Amirhossein/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sklearn \n",
    "import numpy as np\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Let's discover the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import data using pandas and put into SFrames:\n",
    "papers_data = pd.read_csv('Data/output/Papers.csv')\n",
    "authors_data = pd.read_csv('Data/output/Authors.csv')\n",
    "authorId_data = pd.read_csv('Data/output/PaperAuthors.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Title</th>\n",
       "      <th>EventType</th>\n",
       "      <th>PdfName</th>\n",
       "      <th>Abstract</th>\n",
       "      <th>PaperText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5677</td>\n",
       "      <td>Double or Nothing: Multiplicative Incentive Me...</td>\n",
       "      <td>Poster</td>\n",
       "      <td>5677-double-or-nothing-multiplicative-incentiv...</td>\n",
       "      <td>Crowdsourcing has gained immense popularity in...</td>\n",
       "      <td>Double or Nothing: Multiplicative\\nIncentive M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5941</td>\n",
       "      <td>Learning with Symmetric Label Noise: The Impor...</td>\n",
       "      <td>Spotlight</td>\n",
       "      <td>5941-learning-with-symmetric-label-noise-the-i...</td>\n",
       "      <td>Convex potential minimisation is the de facto ...</td>\n",
       "      <td>Learning with Symmetric Label Noise: The\\nImpo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6019</td>\n",
       "      <td>Algorithmic Stability and Uniform Generalization</td>\n",
       "      <td>Poster</td>\n",
       "      <td>6019-algorithmic-stability-and-uniform-general...</td>\n",
       "      <td>One of the central questions in statistical le...</td>\n",
       "      <td>Algorithmic Stability and Uniform Generalizati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6035</td>\n",
       "      <td>Adaptive Low-Complexity Sequential Inference f...</td>\n",
       "      <td>Poster</td>\n",
       "      <td>6035-adaptive-low-complexity-sequential-infere...</td>\n",
       "      <td>We develop a sequential low-complexity inferen...</td>\n",
       "      <td>Adaptive Low-Complexity Sequential Inference f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5978</td>\n",
       "      <td>Covariance-Controlled Adaptive Langevin Thermo...</td>\n",
       "      <td>Poster</td>\n",
       "      <td>5978-covariance-controlled-adaptive-langevin-t...</td>\n",
       "      <td>Monte Carlo sampling for Bayesian posterior in...</td>\n",
       "      <td>Covariance-Controlled Adaptive Langevin\\nTherm...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Id                                              Title  EventType  \\\n",
       "0  5677  Double or Nothing: Multiplicative Incentive Me...     Poster   \n",
       "1  5941  Learning with Symmetric Label Noise: The Impor...  Spotlight   \n",
       "2  6019   Algorithmic Stability and Uniform Generalization     Poster   \n",
       "3  6035  Adaptive Low-Complexity Sequential Inference f...     Poster   \n",
       "4  5978  Covariance-Controlled Adaptive Langevin Thermo...     Poster   \n",
       "\n",
       "                                             PdfName  \\\n",
       "0  5677-double-or-nothing-multiplicative-incentiv...   \n",
       "1  5941-learning-with-symmetric-label-noise-the-i...   \n",
       "2  6019-algorithmic-stability-and-uniform-general...   \n",
       "3  6035-adaptive-low-complexity-sequential-infere...   \n",
       "4  5978-covariance-controlled-adaptive-langevin-t...   \n",
       "\n",
       "                                            Abstract  \\\n",
       "0  Crowdsourcing has gained immense popularity in...   \n",
       "1  Convex potential minimisation is the de facto ...   \n",
       "2  One of the central questions in statistical le...   \n",
       "3  We develop a sequential low-complexity inferen...   \n",
       "4  Monte Carlo sampling for Bayesian posterior in...   \n",
       "\n",
       "                                           PaperText  \n",
       "0  Double or Nothing: Multiplicative\\nIncentive M...  \n",
       "1  Learning with Symmetric Label Noise: The\\nImpo...  \n",
       "2  Algorithmic Stability and Uniform Generalizati...  \n",
       "3  Adaptive Low-Complexity Sequential Inference f...  \n",
       "4  Covariance-Controlled Adaptive Langevin\\nTherm...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4113</td>\n",
       "      <td>Constantine Caramanis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4828</td>\n",
       "      <td>Richard L. Lewis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5506</td>\n",
       "      <td>Ryan Kiros</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7331</td>\n",
       "      <td>Kfir Levy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8429</td>\n",
       "      <td>Wei Cao</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Id                   Name\n",
       "0  4113  Constantine Caramanis\n",
       "1  4828       Richard L. Lewis\n",
       "2  5506             Ryan Kiros\n",
       "3  7331              Kfir Levy\n",
       "4  8429                Wei Cao"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "authors_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>PaperId</th>\n",
       "      <th>AuthorId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5677</td>\n",
       "      <td>7956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>5677</td>\n",
       "      <td>2649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>5941</td>\n",
       "      <td>8299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>5941</td>\n",
       "      <td>8300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5941</td>\n",
       "      <td>575</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  PaperId  AuthorId\n",
       "0   1     5677      7956\n",
       "1   2     5677      2649\n",
       "2   3     5941      8299\n",
       "3   4     5941      8300\n",
       "4   5     5941       575"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "authorId_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define two functions for being able to go from index to id and visa-versa on papers_data: \n",
    "1. A function that takes paper_id and papers_data as input and gives its index\n",
    "2. A function that takes index as input and gives its paper_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def given_paperID_give_index(paper_id, paper_data):\n",
    "    return paper_data[paper_data['Id']==paper_id].index[0]\n",
    "#\n",
    "def given_index_give_PaperID(index, paper_data):\n",
    "    return paper_data.iloc[index]['Id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's look at second paper as an example before cleaning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Learning with Symmetric Label Noise: The\\nImportance of Being Unhinged\\n\\nBrendan van Rooyen\\xe2\\x88\\x97,\\xe2\\x80\\xa0\\n\\xe2\\x88\\x97\\n\\nAditya Krishna Menon\\xe2\\x80\\xa0,\\xe2\\x88\\x97\\n\\nThe Australian National University\\n\\n\\xe2\\x80\\xa0\\n\\nRobert C. Williamson\\xe2\\x88\\x97,\\xe2\\x80\\xa0\\n\\nNational ICT Australia\\n\\n{ brendan.vanrooyen, aditya.menon, bob.williamson }@nicta.com.au\\n\\nAbstract\\nConvex potential minimisation is the de facto approach to binary classification.\\nHowever, Long and Servedio [2010] proved that under symmetric label noise\\n(SLN), minimisation of any convex potential over a linear function class can result in classification performance equivalent to random guessing. This ostensibly\\nshows that convex losses are not SLN-robust. In this paper, we propose a convex,\\nclassification-calibrated loss and prove that it is SLN-robust. The loss avoids the\\nLong and Servedio [2010] result by virtue of being negatively unbounded. The\\nloss is a modification of the hinge loss, where one does not clamp at zero; hence,\\nwe call it the unhinged loss. We show that the optimal unhi'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ex_paper_id = 5941\n",
    "Ex_paper_index = given_paperID_give_index(Ex_paper_id, papers_data)\n",
    "papers_data.iloc[Ex_paper_index]['PaperText'][0:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Abstract and PaperText:\n",
    "* Clean text from \\n \\x and things like that by \n",
    "    1. Replace \\n and \\x0c with space\n",
    "    2. Apply unicode\n",
    "    3. Make everything lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    list_of_cleaning_signs = ['\\x0c', '\\n']\n",
    "    for sign in list_of_cleaning_signs:\n",
    "        text = text.replace(sign, ' ')\n",
    "    text = unicode(text, errors='ignore')\n",
    "    clean_text = re.sub('[^a-zA-Z]+', ' ', text)\n",
    "    return clean_text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "papers_data['PaperText_clean'] = papers_data['PaperText'].apply(lambda x: clean_text(x))\n",
    "papers_data['Abstract_clean'] = papers_data['Abstract'].apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's look at the example paper after cleaning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'learning with symmetric label noise the importance of being unhinged brendan van rooyen aditya krishna menon the australian national university robert c williamson national ict australia brendan vanrooyen aditya menon bob williamson nicta com au abstract convex potential minimisation is the de facto approach to binary classification however long and servedio proved that under symmetric label noise sln minimisation of any convex potential over a linear function class can result in classification performance equivalent to random guessing this ostensibly shows that convex losses are not sln robust in this paper we propose a convex classification calibrated loss and prove that it is sln robust the loss avoids the long and servedio result by virtue of being negatively unbounded the loss is a modification of the hinge loss where one does not clamp at zero hence we call it the unhinged loss we show that the optimal unhinged solution is equivalent to that of a strongly regularised svm and is t'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example After Cleaning\n",
    "papers_data.iloc[1]['PaperText_clean'][0:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build tf-idf matrix based on Abstract & PaperText:\n",
    "* Using Token and Stem [Thanks to the great post by Brandon Rose: http://brandonrose.org/clustering]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# here Brandon defines a tokenizer and stemmer which returns the set of stems in the text that it is passed\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "def tokenize_and_stem(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# Producing tf_idf matrix separately based on Abstract\n",
    "tfidf_vectorizer_Abstract = TfidfVectorizer(max_df=0.95, max_features=200000,\n",
    "                                 min_df=0.05, stop_words='english',\n",
    "                                 use_idf=True, tokenizer=tokenize_and_stem, ngram_range=(1,3))\n",
    "%time tfidf_matrix_Abstract = tfidf_vectorizer_Abstract.fit_transform(papers_data['Abstract_clean'])\n",
    "\n",
    "# Producing tf_idf matrix separately based on PaperText\n",
    "tfidf_vectorizer_PaperText = TfidfVectorizer(max_df=0.9, max_features=200000,\n",
    "                                 min_df=0.1, stop_words='english',\n",
    "                                 use_idf=True, tokenizer=tokenize_and_stem, ngram_range=(1,3))\n",
    "%time tfidf_matrix_PaperText = tfidf_vectorizer_PaperText.fit_transform(papers_data['PaperText_clean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "terms_Abstract = tfidf_vectorizer_Abstract.get_feature_names()\n",
    "terms_PaperText = tfidf_vectorizer_Abstract.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's create a function that takes a paper_id and tfidf_matrix and gives n-important keywords:\n",
    "* [Thanks to the great post by Thomas Buhrmann: https://buhrmann.github.io/tfidf-analysis.html]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def top_tfidf_feats(row, terms, top_n=25):\n",
    "    topn_ids = np.argsort(row)[::-1][:top_n]\n",
    "    top_feats = [(terms[i], row[i]) for i in topn_ids]\n",
    "    df = pd.DataFrame(top_feats)\n",
    "    df.columns = ['feature', 'tfidf']\n",
    "    return df['feature']\n",
    "def given_paperID_give_keywords(paper_data, tfidfMatrix, terms, paper_id, top_n=20):\n",
    "    row_id = given_paperID_give_index(paper_id, paper_data)\n",
    "    row = np.squeeze(tfidfMatrix[row_id].toarray())\n",
    "    return top_tfidf_feats(row, terms, top_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's check the top 10-keywords of the example paper based on Abstract:\n",
    "Note: The words are in stemmed form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "paper_id_example = 5941\n",
    "print \"Keywords based on Abstract:\"\n",
    "print given_paperID_give_keywords(papers_data, tfidf_matrix_Abstract, terms_Abstract, paper_id_example, top_n = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build NearestNeighbors models based on Abstract and PaperText:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "# Based on Abstract\n",
    "num_neighbors = 4\n",
    "nbrs_Abstract = NearestNeighbors(n_neighbors=num_neighbors,  algorithm='auto').fit(tfidf_matrix_Abstract)\n",
    "distances_Abstract, indices_Abstract = nbrs_Abstract.kneighbors(tfidf_matrix_Abstract)\n",
    "# Based on PaperText\n",
    "nbrs_PaperText = NearestNeighbors(n_neighbors=num_neighbors,  algorithm='auto').fit(tfidf_matrix_PaperText)\n",
    "distances_PaperText, indices_PaperText = nbrs_PaperText.kneighbors(tfidf_matrix_PaperText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Nbrs of the example paper based on Abstract similarity: %r\" % indices_Abstract[1]\n",
    "print \"Nbrs of the example paper based on PaperText similarity: %r\" % indices_PaperText[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's check the abstract of the similar papers found for the example paper mentioned above:\n",
    "* a) Using model based on Abstract\n",
    "* b) Using model based on PaperText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Ex_paper_id = 5941\n",
    "Ex_index = given_paperID_give_index(Ex_paper_id, papers_data)\n",
    "print \"The Abstract of the example paper is:\\n\"\n",
    "print papers_data.iloc[indices_Abstract[Ex_index][0]]['Abstract']\n",
    "print \"The Abstract of the similar papers are:\\n\"\n",
    "for i in xrange(1, len(indices_Abstract[Ex_index])):\n",
    "    print \"Neighbor No. %r has following abstract: \\n\" % i\n",
    "    print papers_data.iloc[indices_Abstract[Ex_index][i]]['Abstract']\n",
    "    print \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Ex_paper_id = 5941\n",
    "Ex_index = given_paperID_give_index(Ex_paper_id, papers_data)\n",
    "print \"The Abstract of the example paper is:\\n\"\n",
    "print papers_data.iloc[indices_PaperText[Ex_index][0]]['Abstract']\n",
    "print \"The Abstract of the similar papers are:\\n\"\n",
    "for i in xrange(1, len(indices_PaperText[Ex_index])):\n",
    "    print \"Neighbor No. %r has following abstract: \\n\" % i\n",
    "    print papers_data.iloc[indices_PaperText[Ex_index][i]]['Abstract']\n",
    "    print \"\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some post-processing functions to help us read author's names and title of their papers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def given_paperID_give_authours_id(paper_id, author_data, author_id_data):\n",
    "    id_author_list = author_id_data[author_id_data['PaperId']==paper_id]['AuthorId']\n",
    "    return id_author_list\n",
    "\n",
    "def given_authorID_give_name(author_id, author_data):\n",
    "    author_name = author_data[author_data['Id'] == author_id]['Name']\n",
    "    return author_name\n",
    "\n",
    "def given_similar_paperIDs_give_their_titles(sim_papers_list_index, paper_data):\n",
    "    titles = []\n",
    "    for index in sim_papers_list_index:\n",
    "        titles.append(paper_data.iloc[index]['Title']+'.')\n",
    "    return titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Ex_paper_id = 5941\n",
    "Ex_index = given_paperID_give_index(Ex_paper_id, papers_data)\n",
    "print \"Title of similar papers to the example paper based on Abstract:\\n\\n\"\n",
    "for title in given_similar_paperIDs_give_their_titles(indices_Abstract[Ex_index], papers_data):\n",
    "    print title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Ex_paper_id = 5941\n",
    "Ex_index = given_paperID_give_index(Ex_paper_id, papers_data)\n",
    "print \"Title of similar papers to the example paper based on Abstract:\\n\\n\"\n",
    "for title in given_similar_paperIDs_give_their_titles(indices_PaperText[Ex_index], papers_data):\n",
    "    print title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *** Questions & notes: ***\n",
    "1. Are these papers really similar? i.e. Is there an automated way to evaluate?\n",
    "    * Maybe we can check if the recommended similar papers referenced the same papers? \n",
    "\n",
    "\n",
    "3. Which model is better? Abstract or PaperText? Which papers are recommended by both models? Are these more similar?\n",
    "\n",
    "4. Try different parameters in generating tf-idf and/or different algorithms in producing the knn model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *** References: ***\n",
    "1. http://brandonrose.org/clustering\n",
    "2. https://buhrmann.github.io/tfidf-analysis.html\n",
    "3. \"Machine Learning Foundations: A Case Study Approach\" course on Coursera"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
