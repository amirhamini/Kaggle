{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Which NIPS papers are similar? *[a simple sklearn knn & tf-idf exercise]*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal: Find the papers that are similar based on abstract and full-text\n",
    "### Steps:\n",
    "1. Find the important keywords of each document using tf-idf\n",
    "2. Apply knn_model on tf-idf to find similar papers\n",
    "\n",
    "### Cleaning: \n",
    "* Clean text from \\n \\x and things like that by \n",
    "    1. Replace \\n and \\x0c with space\n",
    "    2. Apply unicode\n",
    "    3. Make everything lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/Amirhossein/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sklearn \n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Let's discover the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import data using pandas and put into SFrames:\n",
    "papers_data = pd.read_csv('Data/output/Papers.csv')\n",
    "authors_data = pd.read_csv('Data/output/Authors.csv')\n",
    "authorId_data = pd.read_csv('Data/output/PaperAuthors.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Title</th>\n",
       "      <th>EventType</th>\n",
       "      <th>PdfName</th>\n",
       "      <th>Abstract</th>\n",
       "      <th>PaperText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5677</td>\n",
       "      <td>Double or Nothing: Multiplicative Incentive Me...</td>\n",
       "      <td>Poster</td>\n",
       "      <td>5677-double-or-nothing-multiplicative-incentiv...</td>\n",
       "      <td>Crowdsourcing has gained immense popularity in...</td>\n",
       "      <td>Double or Nothing: Multiplicative\\nIncentive M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5941</td>\n",
       "      <td>Learning with Symmetric Label Noise: The Impor...</td>\n",
       "      <td>Spotlight</td>\n",
       "      <td>5941-learning-with-symmetric-label-noise-the-i...</td>\n",
       "      <td>Convex potential minimisation is the de facto ...</td>\n",
       "      <td>Learning with Symmetric Label Noise: The\\nImpo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6019</td>\n",
       "      <td>Algorithmic Stability and Uniform Generalization</td>\n",
       "      <td>Poster</td>\n",
       "      <td>6019-algorithmic-stability-and-uniform-general...</td>\n",
       "      <td>One of the central questions in statistical le...</td>\n",
       "      <td>Algorithmic Stability and Uniform Generalizati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6035</td>\n",
       "      <td>Adaptive Low-Complexity Sequential Inference f...</td>\n",
       "      <td>Poster</td>\n",
       "      <td>6035-adaptive-low-complexity-sequential-infere...</td>\n",
       "      <td>We develop a sequential low-complexity inferen...</td>\n",
       "      <td>Adaptive Low-Complexity Sequential Inference f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5978</td>\n",
       "      <td>Covariance-Controlled Adaptive Langevin Thermo...</td>\n",
       "      <td>Poster</td>\n",
       "      <td>5978-covariance-controlled-adaptive-langevin-t...</td>\n",
       "      <td>Monte Carlo sampling for Bayesian posterior in...</td>\n",
       "      <td>Covariance-Controlled Adaptive Langevin\\nTherm...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Id                                              Title  EventType  \\\n",
       "0  5677  Double or Nothing: Multiplicative Incentive Me...     Poster   \n",
       "1  5941  Learning with Symmetric Label Noise: The Impor...  Spotlight   \n",
       "2  6019   Algorithmic Stability and Uniform Generalization     Poster   \n",
       "3  6035  Adaptive Low-Complexity Sequential Inference f...     Poster   \n",
       "4  5978  Covariance-Controlled Adaptive Langevin Thermo...     Poster   \n",
       "\n",
       "                                             PdfName  \\\n",
       "0  5677-double-or-nothing-multiplicative-incentiv...   \n",
       "1  5941-learning-with-symmetric-label-noise-the-i...   \n",
       "2  6019-algorithmic-stability-and-uniform-general...   \n",
       "3  6035-adaptive-low-complexity-sequential-infere...   \n",
       "4  5978-covariance-controlled-adaptive-langevin-t...   \n",
       "\n",
       "                                            Abstract  \\\n",
       "0  Crowdsourcing has gained immense popularity in...   \n",
       "1  Convex potential minimisation is the de facto ...   \n",
       "2  One of the central questions in statistical le...   \n",
       "3  We develop a sequential low-complexity inferen...   \n",
       "4  Monte Carlo sampling for Bayesian posterior in...   \n",
       "\n",
       "                                           PaperText  \n",
       "0  Double or Nothing: Multiplicative\\nIncentive M...  \n",
       "1  Learning with Symmetric Label Noise: The\\nImpo...  \n",
       "2  Algorithmic Stability and Uniform Generalizati...  \n",
       "3  Adaptive Low-Complexity Sequential Inference f...  \n",
       "4  Covariance-Controlled Adaptive Langevin\\nTherm...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4113</td>\n",
       "      <td>Constantine Caramanis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4828</td>\n",
       "      <td>Richard L. Lewis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5506</td>\n",
       "      <td>Ryan Kiros</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7331</td>\n",
       "      <td>Kfir Levy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8429</td>\n",
       "      <td>Wei Cao</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Id                   Name\n",
       "0  4113  Constantine Caramanis\n",
       "1  4828       Richard L. Lewis\n",
       "2  5506             Ryan Kiros\n",
       "3  7331              Kfir Levy\n",
       "4  8429                Wei Cao"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "authors_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>PaperId</th>\n",
       "      <th>AuthorId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5677</td>\n",
       "      <td>7956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>5677</td>\n",
       "      <td>2649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>5941</td>\n",
       "      <td>8299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>5941</td>\n",
       "      <td>8300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5941</td>\n",
       "      <td>575</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  PaperId  AuthorId\n",
       "0   1     5677      7956\n",
       "1   2     5677      2649\n",
       "2   3     5941      8299\n",
       "3   4     5941      8300\n",
       "4   5     5941       575"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "authorId_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define two functions for being able to go from index to id and visa-versa on papers_data: \n",
    "1. A function that takes paper_id and papers_data as input and gives its index\n",
    "2. A function that takes index as input and gives its paper_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def given_paperID_give_index(paper_id, paper_data):\n",
    "    return paper_data[paper_data['Id']==paper_id].index[0]\n",
    "#\n",
    "def given_index_give_PaperID(index, paper_data):\n",
    "    return paper_data.iloc[index]['Id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's look at second paper as an example before cleaning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Learning with Symmetric Label Noise: The\\nImportance of Being Unhinged\\n\\nBrendan van Rooyen\\xe2\\x88\\x97,\\xe2\\x80\\xa0\\n\\xe2\\x88\\x97\\n\\nAditya Krishna Menon\\xe2\\x80\\xa0,\\xe2\\x88\\x97\\n\\nThe Australian National University\\n\\n\\xe2\\x80\\xa0\\n\\nRobert C. Williamson\\xe2\\x88\\x97,\\xe2\\x80\\xa0\\n\\nNational ICT Australia\\n\\n{ brendan.vanrooyen, aditya.menon, bob.williamson }@nicta.com.au\\n\\nAbstract\\nConvex potential minimisation is the de facto approach to binary classification.\\nHowever, Long and Servedio [2010] proved that under symmetric label noise\\n(SLN), minimisation of any convex potential over a linear function class can result in classification performance equivalent to random guessing. This ostensibly\\nshows that convex losses are not SLN-robust. In this paper, we propose a convex,\\nclassification-calibrated loss and prove that it is SLN-robust. The loss avoids the\\nLong and Servedio [2010] result by virtue of being negatively unbounded. The\\nloss is a modification of the hinge loss, where one does not clamp at zero; hence,\\nwe call it the unhinged loss. We show that the optimal unhi'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ex_paper_id = 5941\n",
    "Ex_paper_index = given_paperID_give_index(Ex_paper_id, papers_data)\n",
    "papers_data.iloc[Ex_paper_index]['PaperText'][0:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Abstract and PaperText:\n",
    "* Clean text from \\n \\x and things like that by \n",
    "    1. Replace \\n and \\x0c with space\n",
    "    2. Apply unicode\n",
    "    3. Make everything lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    list_of_cleaning_signs = ['\\x0c', '\\n']\n",
    "    for sign in list_of_cleaning_signs:\n",
    "        text = text.replace(sign, ' ')\n",
    "    text = unicode(text, errors='ignore')\n",
    "    clean_text = re.sub('[^a-zA-Z]+', ' ', text)\n",
    "    return clean_text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "papers_data['PaperText_clean'] = papers_data['PaperText'].apply(lambda x: clean_text(x))\n",
    "papers_data['Abstract_clean'] = papers_data['Abstract'].apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's look at the example paper after cleaning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'learning with symmetric label noise the importance of being unhinged brendan van rooyen aditya krishna menon the australian national university robert c williamson national ict australia brendan vanrooyen aditya menon bob williamson nicta com au abstract convex potential minimisation is the de facto approach to binary classification however long and servedio proved that under symmetric label noise sln minimisation of any convex potential over a linear function class can result in classification performance equivalent to random guessing this ostensibly shows that convex losses are not sln robust in this paper we propose a convex classification calibrated loss and prove that it is sln robust the loss avoids the long and servedio result by virtue of being negatively unbounded the loss is a modification of the hinge loss where one does not clamp at zero hence we call it the unhinged loss we show that the optimal unhinged solution is equivalent to that of a strongly regularised svm and is t'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example After Cleaning\n",
    "papers_data.iloc[1]['PaperText_clean'][0:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build tf-idf matrix based on Abstract & PaperText:\n",
    "* Using Token and Stem [Thanks to the great post by Brandon Rose: http://brandonrose.org/clustering]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# here Brandon defines a tokenizer and stemmer which returns the set of stems in the text that it is passed\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "def tokenize_and_stem(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.91 s, sys: 85 ms, total: 3.99 s\n",
      "Wall time: 4.15 s\n",
      "CPU times: user 1min 45s, sys: 1.89 s, total: 1min 47s\n",
      "Wall time: 1min 48s\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# Producing tf_idf matrix separately based on Abstract\n",
    "tfidf_vectorizer_Abstract = TfidfVectorizer(max_df=0.95, max_features=200000,\n",
    "                                 min_df=0.05, stop_words='english',\n",
    "                                 use_idf=True, tokenizer=tokenize_and_stem, ngram_range=(1,3))\n",
    "%time tfidf_matrix_Abstract = tfidf_vectorizer_Abstract.fit_transform(papers_data['Abstract_clean'])\n",
    "\n",
    "# Producing tf_idf matrix separately based on PaperText\n",
    "tfidf_vectorizer_PaperText = TfidfVectorizer(max_df=0.9, max_features=200000,\n",
    "                                 min_df=0.1, stop_words='english',\n",
    "                                 use_idf=True, tokenizer=tokenize_and_stem, ngram_range=(1,3))\n",
    "%time tfidf_matrix_PaperText = tfidf_vectorizer_PaperText.fit_transform(papers_data['PaperText_clean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "terms_Abstract = tfidf_vectorizer_Abstract.get_feature_names()\n",
    "terms_PaperText = tfidf_vectorizer_Abstract.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's create a function that takes a paper_id and tfidf_matrix and gives n-important keywords:\n",
    "* [Thanks to the great post by Thomas Buhrmann: https://buhrmann.github.io/tfidf-analysis.html]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def top_tfidf_feats(row, terms, top_n=25):\n",
    "    topn_ids = np.argsort(row)[::-1][:top_n]\n",
    "    top_feats = [(terms[i], row[i]) for i in topn_ids]\n",
    "    df = pd.DataFrame(top_feats)\n",
    "    df.columns = ['feature', 'tfidf']\n",
    "    return df['feature']\n",
    "def given_paperID_give_keywords(paper_data, tfidfMatrix, terms, paper_id, top_n=20):\n",
    "    row_id = given_paperID_give_index(paper_id, paper_data)\n",
    "    row = np.squeeze(tfidfMatrix[row_id].toarray())\n",
    "    return top_tfidf_feats(row, terms, top_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's check the top 10-keywords of the example paper based on Abstract:\n",
    "Note: The words are in stemmed form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keywords based on Abstract:\n",
      "0            loss\n",
      "1          convex\n",
      "2          robust\n",
      "3         classif\n",
      "4          strong\n",
      "5           solut\n",
      "6           prove\n",
      "7             ani\n",
      "8    paper propos\n",
      "9          result\n",
      "Name: feature, dtype: object\n"
     ]
    }
   ],
   "source": [
    "paper_id_example = 5941\n",
    "print \"Keywords based on Abstract:\"\n",
    "print given_paperID_give_keywords(papers_data, tfidf_matrix_Abstract, terms_Abstract, paper_id_example, top_n = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build NearestNeighbors models based on Abstract and PaperText:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "# Based on Abstract\n",
    "num_neighbors = 4\n",
    "nbrs_Abstract = NearestNeighbors(n_neighbors=num_neighbors,  algorithm='auto').fit(tfidf_matrix_Abstract)\n",
    "distances_Abstract, indices_Abstract = nbrs_Abstract.kneighbors(tfidf_matrix_Abstract)\n",
    "# Based on PaperText\n",
    "nbrs_PaperText = NearestNeighbors(n_neighbors=num_neighbors,  algorithm='auto').fit(tfidf_matrix_PaperText)\n",
    "distances_PaperText, indices_PaperText = nbrs_PaperText.kneighbors(tfidf_matrix_PaperText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nbrs of the example paper based on Abstract similarity: array([  1,  87, 301, 112])\n",
      "Nbrs of the example paper based on PaperText similarity: array([  1, 125, 112, 148])\n"
     ]
    }
   ],
   "source": [
    "print \"Nbrs of the example paper based on Abstract similarity: %r\" % indices_Abstract[1]\n",
    "print \"Nbrs of the example paper based on PaperText similarity: %r\" % indices_PaperText[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's check the abstract of the similar papers found for the example paper mentioned above:\n",
    "* a) Using model based on Abstract\n",
    "* b) Using model based on PaperText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Abstract of the example paper is:\n",
      "\n",
      "Convex potential minimisation is the de facto approach to binary classification. However, Long and Servedio [2008] proved that under symmetric label noise (SLN), minimisation of any convex potential over a linear function class can result in classification performance equivalent to random guessing. This ostensibly shows that convex losses are not SLN-robust. In this paper, we propose a convex, classification-calibrated loss and prove that it is SLN-robust. The loss avoids the Long and Servedio [2008] result by virtue of being negatively unbounded. The loss is a modification of the hinge loss, where one does not clamp at zero; hence, we call it the unhinged loss. We show that the optimal unhinged solution is equivalent to that of a strongly regularised SVM, and is the limiting solution for any convex potential; this implies that strong l2 regularisation makes most standard learners SLN-robust. Experiments confirm the unhinged loss’ SLN-robustness.\n",
      "The Abstract of the similar papers are:\n",
      "\n",
      "Neighbor No. 1 has following abstract: \n",
      "\n",
      "The framework of online learning with memory naturally captures learning problems with temporal effects, and was previously studied for the experts setting. In this work we extend the notion of learning with memory to the general Online Convex Optimization (OCO) framework, and present two algorithms that attain low regret. The first algorithm applies to Lipschitz continuous loss functions, obtaining optimal regret bounds for both convex and strongly convex losses. The second algorithm attains the optimal regret bounds and applies more broadly to convex losses without requiring Lipschitz continuity, yet is more complicated to implement. We complement the theoretic results with two applications: statistical arbitrage in finance, and multi-step ahead prediction in statistics.\n",
      "\n",
      "\n",
      "Neighbor No. 2 has following abstract: \n",
      "\n",
      "Multivariate loss functions are used to assess performance in many modern prediction tasks, including information retrieval and ranking applications. Convex approximations are typically optimized in their place to avoid NP-hard empirical risk minimization problems. We propose to approximate the training data instead of the loss function by posing multivariate prediction as an adversarial game between a loss-minimizing prediction player and a loss-maximizing evaluation player constrained to match specified properties of training data. This avoids the non-convexity of empirical risk minimization, but game sizes are exponential in the number of predicted variables. We overcome this intractability using the double oracle constraint generation method. We demonstrate the efficiency and predictive performance of our approach on tasks evaluated using the precision at k, the F-score and the discounted cumulative gain.\n",
      "\n",
      "\n",
      "Neighbor No. 3 has following abstract: \n",
      "\n",
      "Modern prediction problems arising in multilabel learning and learning to rank pose unique challenges to the classical theory of supervised learning. These problems have large prediction and label spaces of a combinatorial nature and involve sophisticated loss functions. We offer a general framework to derive mistake driven online algorithms and associated loss bounds.  The key ingredients in our framework are a general loss function, a general vector space representation of predictions, and a notion of margin with respect to a general norm. Our general algorithm, Predtron, yields the perceptron algorithm and its variants when instantiated on classic problems such as binary classification, multiclass classification, ordinal regression, and multilabel classification.  For multilabel ranking and subset ranking, we derive novel algorithms, notions of margins, and loss bounds. A simulation study confirms the behavior predicted by our bounds and demonstrates the flexibility of the design choices in our framework.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Ex_paper_id = 5941\n",
    "Ex_index = given_paperID_give_index(Ex_paper_id, papers_data)\n",
    "print \"The Abstract of the example paper is:\\n\"\n",
    "print papers_data.iloc[indices_Abstract[Ex_index][0]]['Abstract']\n",
    "print \"The Abstract of the similar papers are:\\n\"\n",
    "for i in xrange(1, len(indices_Abstract[Ex_index])):\n",
    "    print \"Neighbor No. %r has following abstract: \\n\" % i\n",
    "    print papers_data.iloc[indices_Abstract[Ex_index][i]]['Abstract']\n",
    "    print \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Abstract of the example paper is:\n",
      "\n",
      "Convex potential minimisation is the de facto approach to binary classification. However, Long and Servedio [2008] proved that under symmetric label noise (SLN), minimisation of any convex potential over a linear function class can result in classification performance equivalent to random guessing. This ostensibly shows that convex losses are not SLN-robust. In this paper, we propose a convex, classification-calibrated loss and prove that it is SLN-robust. The loss avoids the Long and Servedio [2008] result by virtue of being negatively unbounded. The loss is a modification of the hinge loss, where one does not clamp at zero; hence, we call it the unhinged loss. We show that the optimal unhinged solution is equivalent to that of a strongly regularised SVM, and is the limiting solution for any convex potential; this implies that strong l2 regularisation makes most standard learners SLN-robust. Experiments confirm the unhinged loss’ SLN-robustness.\n",
      "The Abstract of the similar papers are:\n",
      "\n",
      "Neighbor No. 1 has following abstract: \n",
      "\n",
      "In regression problems involving vector-valued outputs (or equivalently, multiple responses), it is well known that the maximum likelihood estimator (MLE), which takes noise covariance structure into account, can be significantly more accurate than the ordinary least squares (OLS) estimator. However, existing  literature compares OLS and MLE in terms of their asymptotic, not finite sample, guarantees. More crucially, computing the MLE in general requires solving a non-convex optimization problem and is not known to be efficiently solvable. We provide finite sample upper and lower bounds on the estimation error of OLS and MLE, in two popular models: a) Pooled model, b) Seemingly Unrelated Regression (SUR) model. We provide precise instances where the MLE is significantly more accurate than OLS. Furthermore, for both models, we show that the output of a computationally efficient alternating minimization procedure enjoys the same performance guarantee as MLE, up to universal constants. Finally, we show that for high-dimensional settings as well, the alternating minimization procedure leads to significantly more accurate solutions than the corresponding OLS solutions but with error bound that depends only logarithmically on the data dimensionality.\n",
      "\n",
      "\n",
      "Neighbor No. 2 has following abstract: \n",
      "\n",
      "Modern prediction problems arising in multilabel learning and learning to rank pose unique challenges to the classical theory of supervised learning. These problems have large prediction and label spaces of a combinatorial nature and involve sophisticated loss functions. We offer a general framework to derive mistake driven online algorithms and associated loss bounds.  The key ingredients in our framework are a general loss function, a general vector space representation of predictions, and a notion of margin with respect to a general norm. Our general algorithm, Predtron, yields the perceptron algorithm and its variants when instantiated on classic problems such as binary classification, multiclass classification, ordinal regression, and multilabel classification.  For multilabel ranking and subset ranking, we derive novel algorithms, notions of margins, and loss bounds. A simulation study confirms the behavior predicted by our bounds and demonstrates the flexibility of the design choices in our framework.\n",
      "\n",
      "\n",
      "Neighbor No. 3 has following abstract: \n",
      "\n",
      "This paper proposes a framework for learning features that are robust to data variation, which is particularly important when only a limited number of trainingsamples are available. The framework makes it possible to tradeoff the discriminative value of learned features against the generalization error of the learning algorithm. Robustness is achieved by encouraging the transform that maps data to features to be a local isometry. This geometric property is shown to improve (K, \\epsilon)-robustness, thereby providing theoretical justification for reductions in generalization error observed in experiments. The proposed optimization frameworkis used to train standard learning algorithms such as deep neural networks. Experimental results obtained on benchmark datasets, such as labeled faces in the wild,demonstrate the value of being able to balance discrimination and robustness.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Ex_paper_id = 5941\n",
    "Ex_index = given_paperID_give_index(Ex_paper_id, papers_data)\n",
    "print \"The Abstract of the example paper is:\\n\"\n",
    "print papers_data.iloc[indices_PaperText[Ex_index][0]]['Abstract']\n",
    "print \"The Abstract of the similar papers are:\\n\"\n",
    "for i in xrange(1, len(indices_PaperText[Ex_index])):\n",
    "    print \"Neighbor No. %r has following abstract: \\n\" % i\n",
    "    print papers_data.iloc[indices_PaperText[Ex_index][i]]['Abstract']\n",
    "    print \"\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some post-processing functions to help us read author's names and title of their papers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def given_paperID_give_authours_id(paper_id, author_data, author_id_data):\n",
    "    id_author_list = author_id_data[author_id_data['PaperId']==paper_id]['AuthorId']\n",
    "    return id_author_list\n",
    "\n",
    "def given_authorID_give_name(author_id, author_data):\n",
    "    author_name = author_data[author_data['Id'] == author_id]['Name']\n",
    "    return author_name\n",
    "\n",
    "def given_similar_paperIDs_give_their_titles(sim_papers_list_index, paper_data):\n",
    "    titles = []\n",
    "    for index in sim_papers_list_index:\n",
    "        titles.append(paper_data.iloc[index]['Title']+'.')\n",
    "    return titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title of similar papers to the example paper based on Abstract:\n",
      "\n",
      "\n",
      "Learning with Symmetric Label Noise: The Importance of Being Unhinged.\n",
      "Online Learning for Adversaries with Memory: Price of Past Mistakes.\n",
      "Adversarial Prediction Games for Multivariate Losses.\n",
      "Predtron: A Family of Online Algorithms for General Prediction Problems.\n"
     ]
    }
   ],
   "source": [
    "Ex_paper_id = 5941\n",
    "Ex_index = given_paperID_give_index(Ex_paper_id, papers_data)\n",
    "print \"Title of similar papers to the example paper based on Abstract:\\n\\n\"\n",
    "for title in given_similar_paperIDs_give_their_titles(indices_Abstract[Ex_index], papers_data):\n",
    "    print title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title of similar papers to the example paper based on Abstract:\n",
      "\n",
      "\n",
      "Learning with Symmetric Label Noise: The Importance of Being Unhinged.\n",
      "Alternating Minimization for Regression Problems with Vector-valued Outputs.\n",
      "Predtron: A Family of Online Algorithms for General Prediction Problems.\n",
      "Discriminative Robust Transformation Learning.\n"
     ]
    }
   ],
   "source": [
    "Ex_paper_id = 5941\n",
    "Ex_index = given_paperID_give_index(Ex_paper_id, papers_data)\n",
    "print \"Title of similar papers to the example paper based on Abstract:\\n\\n\"\n",
    "for title in given_similar_paperIDs_give_their_titles(indices_PaperText[Ex_index], papers_data):\n",
    "    print title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *** Questions & notes: ***\n",
    "1. Are these papers really similar? i.e. Is there an automated way to evaluate?\n",
    "    * Maybe we can check if the recommended similar papers referenced the same papers? \n",
    "\n",
    "\n",
    "3. Which model is better? Abstract or PaperText? Which papers are recommended by both models? Are these more similar?\n",
    "\n",
    "4. Try different parameters in generating tf-idf and/or different algorithms in producing the knn model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *** References: ***\n",
    "1. http://brandonrose.org/clustering\n",
    "2. https://buhrmann.github.io/tfidf-analysis.html\n",
    "3. \"Machine Learning Foundations: A Case Study Approach\" course on Coursera"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
